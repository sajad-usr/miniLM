# Self-Attention, Transformers, and Pretraining
This repository contains my solutions to assignment 4 of [CS224n](https://web.stanford.edu/class/cs224n/index.html) at Stanford. The assignment can be found [here](https://web.stanford.edu/class/cs224n/index.html#schedule). Note that the starter files are from the course and I just solved the missing parts.

The goal of this assignment is to pretrain a Transformer model on Wikipedia text data, and the finetune it for the downstream task which is predicting birthplace.

## Part (e):
In this part, we do both pretraining and finetuning. The accuracy of the trained model on dev model is **15.8%** which satisfies the question requirements which asks for accuracy of at least 15%.
