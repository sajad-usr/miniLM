# Self-Attention, Transformers, and Pretraining
This repository contains my solutions to assignment 4 of [CS224n](https://web.stanford.edu/class/cs224n/index.html) at Stanford. The assignment can be found [here](https://web.stanford.edu/class/cs224n/index.html#schedule). Note that the starter files are from the course and I just solved the missing parts.

The goal of this assignment is to pretrain a Transformer model on Wikipedia text data, and the finetune it for the downstream task which is predicting birth place.
